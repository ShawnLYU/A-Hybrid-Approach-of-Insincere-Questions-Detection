%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Diaz Essay
% LaTeX Template
% Version 2.0 (13/1/19)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Nicolas Diaz (nsdiaz@uc.cl)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{diazessay} % Font size (can be 10pt, 11pt or 12pt)
\usepackage{mathptmx}
\usepackage{setspace}
% for author-year citation style
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\textbf{Quora Insincere Questions Classification} \\ {\Large\itshape A Sentiment Analysis Focusing on Quora}} % Title and subtitle

\author{\textbf{Mengxuan Lyu, Jinyue Feng} \\ \textit{University of Toronto}} % Author and institution

\date{\today} % Date, use \date{} for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else

% \begin{abstract}
% Morbi tempor congue porta. Proin semper, leo vitae faucibus dictum, metus mauris lacinia lorem, ac congue leo felis eu turpis. Sed nec nunc pellentesque, gravida eros at, porttitor ipsum. Praesent consequat urna a lacus lobortis ultrices eget ac metus. In tempus hendrerit rhoncus. Mauris dignissim turpis id sollicitudin lacinia. Praesent libero tellus, fringilla nec ullamcorper at, ultrices id nulla. Phasellus placerat a tellus a malesuada.
% \end{abstract}

% \hspace*{3.6mm}\textit{Keywords:} lorem, ipsum, dolor, sit amet, lectus % Keywords

% \vspace{30pt} % Vertical whitespace between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------
\doublespacing % double space

\section{Introduction}

Despite Quora's policy of \textit{``Be Nice, Be Respectful''}, misleading or discriminative questions of various types still exist. In this project, we will primarily focus on questions with non-neutral tone, rhetorical questions, discriminative questions and questions that use sexual content as shock value. The goals of this project are: 1) to develop a model that detects such insincere questions to improve the quality of online conversations; 2) to experiment on how different word embeddings and classification techniques suit this particular problem; 3) to present and visualize corpus statistical information about insincerity; 4) to explore the linguistic nature of insincere question expressions. 

Sentiment analysis is a widely researched area in natural language processing, and sarcasm detection is a extensively studied sub-topic of sentiment analysis. We have examined the linguistic backgrounds of both phenomenon and recognized that they share many similarities that we will discuss in details in section 2.1. Therefore, we are motivated to transfer and extend sarcasm detection approaches to our project. 

\subsection{Data}

This project originates from a Kaggle challenge. Kaggle provides a dataset of $1,306,122$ questions with three fields, namely qid (a question id), question\_text (the content of a question) and target (where insincere questions are labeled as 1). We will also be using word embeddings including \textit{Word2Vec, GloVe, ELMo} and \textit{BERT}. 

\subsection{Project Design}

\subsubsection{Textual Space}

This process involves preprocessing of text including tokenization and PoS-tagging, feature extractions such as counting the number of adjectives, and application of machine learning algorithms such as SVM or LR for classification. We would like to analyze features that have the highest impact on insincerity detection to reveal some statistical information with respect to the expression profile of insincere questions. 

\subsubsection{Embedding Space}

This approach requires the usage of libraries and the implementation of CNN-LSTM models using TensorFlow and PyTorch. We will have a closer examination of these techniques in the following literature review. In addition to the attempt of achieving satisfactory predictions, we will also visualize the relationships among word vectors to help explore the semantical information about insincere questions.

\subsubsection{Modularization and Integration}

As we will discuss in the following literature reviews, the combination of different modularized components in a classification process may yield better performance than any module alone. We intend to deploy innovative methods to merge the textual space and embedding space modules. 

\section{Literature Review}

In this literature review, we will first explain the relationship between sarcasm and insincere questions. Then, we will present some feature extraction approaches that are particularly effective for sentiment analysis, and then discuss word-embeddings which will form the major basis of our neural networks, and finally review neural networks models that have achieved state-of-art performance with a special focus on sarcasm detection. 

\subsection{Relations to Sarcasm}

As we have introduced before, a closely related topic to insincere question classification is sarcasm detection, which is a frequently researched area in sentiment analysis\citep{joshi2017}. Sarcastic messages and insincere questions share many similarities in terms of their linguistic nature:

\begin{enumerate}
	\item \textbf{Sentiment Involvement:} Both sarcasm and rhetorical questions involve non-neutral sentiment under the disguise of a propositional or interrogative structure \citep{joshi2017, schmidt1977}. 
	\item \textbf{Presence of Indicative Words:} One type of insincere questions contain expressions of exclusive absoluteness, such as \textit{``if not"}; other rhetorical questions have non-deontic modal verbs and some other special particles that strongly indicate its persuasive, not interrogative, nature \citep{schmidt1977}. Similarly, sarcastic texts also involve indicative words, such as \textit{`like'} in \textit{``like you understand"} \citep{joshi2017}. 
	\item \textbf{Dropped Negation:} Many sarcastic sentences and rhetorical questions are meant to make a negative statement despite the absence of negation words  \citep{joshi2017, schmidt1977}. For instances, ``Having a cold is so fun" means ``Having a cold is not fun at all", and ``Can such a man be innocent?" means ``Such a man cannot be innocent."
	\item \textbf{Intended Victim:} Sarcastic comments can be used to mock a victim, and insincere questions can be discriminative or disrespectful against certain groups of people. The hurtful components in both situations may be implicit or explicit. 
	\item \textbf{Violation of Truthfulness:} Sarcastic comments that violate truthfulness resemble insincere questions that are based on false premises. To understand these languages, the listener needs to know the violation of truthfulness \citep{joshi2017}. This can be a hard problem in natural language processing as it largely depends on knowledge about certain people or certain topics. However, it is also possible the untruthful texts have syntactic or semantic characteristics.  
\end{enumerate}
% Features in textual space vs embedding space

\subsection{Statistics-focused Features}

 Feature selection in sentiment classification follows an established pattern where four types of features are frequently examined, namely term frequencies, parts of speech counts, presence of opinion words, and negations\citep{medhat2014}. A particularly relevant study is \citep{barbosa2010}, where the authors presented a 2-step sentiment analysis method that only utilized an SVM model for machine learning process, but its feature extraction methods allowed for robustness against noisy data. The researchers developed two feature sets designed to provide abstract representations of short texts: meta-features (including PoS tags and prior subjectivity and polarity) and syntactic features (namely frequencies of certain types of characters) \citep{barbosa2010}. The detection process were divided into two parts: subjectivity detection and polarity detection of the subjective texts. The experiment followed a traditional approach where the classifiers with highest information gain were created using Weka and the learning process were completed using SVM algorithm\citep{barbosa2010}. Although this method no longer offers state-of-art performance, we still consider the feature selection methods valuable to our project. We intend to adapt the design in this study to obtain measurements of corpus statistics to feed as features into our neural network models. 

\subsection{Word Embeddings}
Before we feed the data into our models, we need to project it into vectors of real numbers. Therefore, we review some recent techiniques performing word embeddings and present here.

A critical obstacle of language modeling is caused by the curse of dimensionality. To address this problem, in 2003, \citep{bengio2003neural} proposed a feadforwad Neural Network Language Model (NNLM) to learn a dense distributed representation\citep{hinton1986learning} for words, where both the word feature vectors and the parameters of that probability function are learned at the same time. NNLM was designed to learn $f \left( w _ { t } , \cdots , w _ { t - n + 1 } \right) = \hat { P } \left( w _ { t } | w _ { 1 } ^ { t - 1 } \right)$, $w _ { t } \in V$, where $w_1 \cdots w_T$ is the word sequence and $V$ is a large but finite vocabulary. This objective is divided into two steps: firstly the previous N words would be mapped into real vectors by a share projection matrix, which represents each word of the vocabulary with a distributed feature vectors; then the sequence of word feature vectors would be mapped into a probability distribution over $V$. As a result, this model yielded better perplexity compared to N-gram models. and inspired more researchers looking into distributed representations of words.

Recognizing the need to include distributed representations of words which was introduced by \citep{hinton1984distributed}, \citep{mikolov2013efficient} analyzed NNLM and proposed two models to learn the words' continuous vector representations. Originally, NNLM mainly consists of three layers: words are firstly mapped into embeddings at project layer; then embedded vectors would flow into a ordinary hyperbolic tangent hidden layer; finally the output layer represents the probability distribution over $V$. Therefore, most of the computations happen in the output layer. To learn the continuous word representations with higher accuracy and lower computational cost from a very large corpus, the first model Mikolov et al. proposed is the Continuous Bag-of-Words Model (CBOW) which removes the non-linear hidden layer. In CBOW, embedded words would be first projected and then averaged into the same position, after which a output layer is presented with hierarchical softmax. Similar to CBOW, they also provided another model called Continuous Skip-gram Model to predict the context given a word. The result turned out to be promising as the model did not only capture the syntactic regularities, but also reveal subtle semantic information of words.

Another study on word embedding-based features \citep{joshi2016} also provides insights on how to combine word embeddings with feature selection. This work examined how features calculated from word embedding vectors could augment existing feature sets including n-grams, dictionary-based features, syntactical features and pattern-based features. More specifically, the researchers attempted to use word vector distances to detect context incongruity independent of sentiment changes. The results indicated that word embedding-based features enhanced performance \citep{joshi2016}. This method might be useful in our project for two reasons: 1) insincere questions may also contain context incongruity because of conflict between asking a question while making a statement; 2) questions that have discriminative or sexual language may be detected based on the semantic similarities to a set of sensitive keywords.  In this system, known sources of errors include multiple-sense-induced embedding issues, incapability to detect contextual information, and non-sarcastic metaphors\citep{joshi2016}. We expect the latter two issues have lower impact on insincerity detection than sarcasm detection because contextual information and metaphors are less relevant to insincere questions than sarcastic messages. In the following section, we will discuss the relationship between sarcasm and insincere questions in more details. 

\subsection{Neural Networks Models}

% for citation, use \citep{} instead of \cite

Here we review several studies on sarcasm detection and sentiment analysis in the hope of transferring core concepts and methods to insincere question classification.

\citep{poria2017} presented a CNN architecture that contained in parallel a baseline 2-layer CNN model that directly process the text and three pre-trained models covering three types of clues (in particular, sentiments, emotions and personalities) that could be beneficial for sarcasm detection. These pre-trained models were trained on their benchmark datasets and then used for feature extraction on the sarcastic tweets datasets. Although the baseline features (directly extracted from the word vectors) showed better predictions than all pre-trained models, the combination of the baseline and pre-trained models yielded the best performance \citep{poria2017}. The result indicated that compared to completely distributing word representations, modularized components with specific tasks could improved the overall performance of a CNN model. As we discussed in the previous sections, we have established several characteristics that indicate a Quora question to be insincere; therefore, the idea of combining various components each responsible for detecting different classification clues can be valuable for our project. To merge the outputs of the four components, the researchers tested two methods: combining the outputs using SVM and appending extracted features from the pre-trained models into the baseline CNN hidden layers \citep{poria2017}. The former method showed better results possibly because appending the extracted features into the baseline models compromised their meaningfulness. Also, SVM classifiers are known to be ideal for processing text data due to the sparse nature of text \citep{medhat2014}. One additional lesson from this study is that their CNN model with a relatively lower feature dimension (at 1,100) outperformed models using larger vector size (>500,000) both in terms of accuracy and computation speed \citep{poria2017}. 

Another sarcasm detection study \citep{ghosh2016} presented a CNN-LSTM-DNN neural networks model for semantic modeling to detect sarcastic tweets, and this model architecture was particularly relevant to our project design. Consider that the information-bearing parts of a text could exit anywhere, the choice of convolutional neural networks and recurrent neural networks was reasonable as both CNN and RNN were capable of capturing temporal sequential information. Among RNN implementations, LSTMs were beneficial because of the easiness to train and the avoidance of vanishing or exploding gradients. The combination of CNN and RNN networks were also advantageous for the following reasons: 1) convolutional layer helped to extract abstract and compact features to be used as inputs to LSTM network \citep{chan2015}; 2) LSTM compensates for the disadvantage of fixed filter width of convolutional layers, allowing for better analysis of long-distance relationships over texts with various length; 3) CNN could reduce feature variations so that CNN-LSTM worked more efficiently\citep{ghosh2016}. On top of the LSTM layers, the researchers added a fully-connected DNN layer to map the features into a more separable space. The combination of CNN, LSTM, and DNN showed more superior results than recursive SVM, two-layer CNN, and two-layer RNN models\citep{ghosh2016}. Consider the similarity between the tasks of this study and our project, we intend to construct our neural networks architecture following this CNN-LSTM pattern. 

Compared to long texts that contain more contextual information, short texts sentiment analysis can be more challenging and thus methods such as bag-of-words or n-grams are shown to be less effective\citep{barbosa2010, santos2014}. \citep{santos2014} proposed a deep CNN model that constructed word embeddings from character-level to sentence-level for sentiment analysis. Compared to the four-component CNN model in \citep{poria2017} which horizontally extracted information from different aspects, this model specialized in obtaining textual information in depth. The character-level embeddings captured morphological information and the word-level embeddings catch syntactical and semantical information \citep{santos2014}. Finally, the sentence-level representations were constructed upon concatenated character-level and word-level features. The word-level embeddings came from Word2Vec embeddings, and the character-level and sentence-level were obtained using convolutional layers that successively extracted local features in windows and maxed over these windows to get the global representation \citep{santos2014}. According to the results, the extracted features at sentence-level typically concentrated on several sentiment-bearing keywords. On the other hand, the contributions of character-level representations were not closely examined, so to what degree morphological and shape information benefited sentiment analysis were not clear. Our project will primarily focus on semantical and syntactical analysis, but we would also like to explore the advantages of character-level embeddings given enough time and resources. 



%------------------------------------------------

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}

\bibliography{csc2511.bib}

%----------------------------------------------------------------------------------------

\end{document}
