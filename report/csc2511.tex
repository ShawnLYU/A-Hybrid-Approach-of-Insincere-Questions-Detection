%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Diaz Essay
% LaTeX Template
% Version 2.0 (13/1/19)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Nicolas Diaz (nsdiaz@uc.cl)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{diazessay} % Font size (can be 10pt, 11pt or 12pt)
\usepackage{mathptmx}
\usepackage{setspace}
% for author-year citation style
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\textbf{Quora Insincere Questions Classification}} % Title and subtitle

\author{\textbf{Mengxuan Lyu, Jinyue Feng} \\ \textit{University of Toronto}} % Author and institution

\date{\today} % Date, use \date{} for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else

% \begin{abstract}
% Morbi tempor congue porta. Proin semper, leo vitae faucibus dictum, metus mauris lacinia lorem, ac congue leo felis eu turpis. Sed nec nunc pellentesque, gravida eros at, porttitor ipsum. Praesent consequat urna a lacus lobortis ultrices eget ac metus. In tempus hendrerit rhoncus. Mauris dignissim turpis id sollicitudin lacinia. Praesent libero tellus, fringilla nec ullamcorper at, ultrices id nulla. Phasellus placerat a tellus a malesuada.
% \end{abstract}

% \hspace*{3.6mm}\textit{Keywords:} lorem, ipsum, dolor, sit amet, lectus % Keywords

% \vspace{30pt} % Vertical whitespace between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------
\doublespacing % double space

\section{Introduction}

Despite Quora's policy of \textit{``Be Nice, Be Respectful''}, misleading or discriminative questions of various types still exist. In this project, we will primarily focus on questions with non-neutral tone, rhetorical questions, discriminative questions and questions that use sexual content as shock value. After reviewing sentiment analysis studies, we have found that sarcasm detection is an extensively studied topic in this field. However, there are few studies on the classification of insincere or toxic questions. A closer look at the linguistic backgrounds of sarcasm and insincere questions revealed that these two phenomena share many similarities, which we will discuss in details in section \ref{relations-to-sarcasm}. Therefore, we are motivated to transfer and extend sarcasm detection approaches to our project. 

The goals of this project are: 
\begin{enumerate}
\item to develop a model that detects insincere questions to improve the quality of online conversations; 
\item to experiment on how different word embeddings and classification techniques suit this particular problem; 
\item to present and visualize corpus statistical information with an emphasis on how these information affect the classification results; 
\item to explore the linguistic nature of insincere question expressions. 
\end{enumerate}

\subsection{Data}

This project originates from a Kaggle challenge. Kaggle provides a dataset of $1,306,122$ questions with three fields \footnote{https://www.kaggle.com/c/quora-insincere-questions-classification/data}, namely \textit{qid} (a question id), \textit{question\_text} (the content of a question) and \textit{target} (where insincere questions are labeled as 1). We will also be using word embeddings including \textit{Word2Vec, GloVe, ELMo} and \textit{BERT}. 

\subsection{Project Design}

\subsubsection{Text to Vector}

The first part of our project would be mapping text data into numerical space for the further downstream task of classification. Here we adopt three ways of converting.

\begin{itemize}
	\item Textual Space

This process involves preprocessing of texts including tokenization and PoS-tagging, feature extraction such as counting the number of adjectives, and application of machine learning algorithms such as Support Vector Machine (SVM) and Random Forests (RF) for classification. We will analyze features that have the highest impact on insincerity detection to reveal some statistical information concerning the expression profile of insincere questions. 

	\item Embedding Space

This approach requires the usage of libraries and the implementation of CNN-LSTM models using TensorFlow and PyTorch. We will have a closer examination of these techniques in the following literature review. In addition to the attempt of achieving satisfactory predictions, we will also visualize the relationships among word vectors to help explore the semantical information about insincere questions.

	\item Modularization and Integration

As we will discuss in the following sections, the combination of different modularized components in a classification process may yield better performance than any module alone. We intend to deploy innovative methods to merge the textual space and embedding space modules. 	
\end{itemize}

\subsubsection{Classification}

The second part would be implementing different models, including SVM, RF, and nerual networks as well, to perform classification over mapped data.


\section{Literature Review}

In this literature review, we will first explain the relationship between sarcasm and insincere questions (section \ref{relations-to-sarcasm}). Then, we will present some feature extraction approaches that are particularly effective for sentiment analysis (section \ref{statistic-features}) and discuss word-embeddings (section \ref{word-embeddings}) which will form the primary basis of our neural networks. Finally, we will review neural networks models that have achieved state-of-art performance with a particular focus on sarcasm detection (section \ref{nn-models}). 

\subsection{Relations to Sarcasm} \label{relations-to-sarcasm}

As we have introduced before, a closely related topic to insincere question classification is sarcasm detection, which is a frequently researched area in sentiment analysis\citep{joshi2017}. Sarcastic messages and insincere questions share many similarities regarding their linguistic nature:

\begin{enumerate}
	\item \textbf{Sentiment Involvement.} Both sarcasm and rhetorical questions involve non-neutral sentiments under the disguise of a propositional or interrogative structure \citep{joshi2017, schmidt1977}. 
	\item \textbf{Presence of Indicative Words.} One type of insincere questions contain expressions of exclusive absoluteness, such as \textit{``if not"}; other rhetorical questions have non-deontic modal verbs and some other special particles that strongly indicate its persuasive, not interrogative, nature \citep{schmidt1977}. Similarly, sarcastic texts also involve indicative words, such as \textit{`like'} in \textit{``like you understand"} \citep{joshi2017}. 
	\item \textbf{Dropped Negation.} Many sarcastic sentences and rhetorical questions are meant to make a negative statement despite the absence of negation words  \citep{joshi2017, schmidt1977}. For instances, ``Having a cold is so fun" means ``Having a cold is not fun at all", and ``Can such a man be innocent?" means ``Such a man cannot be innocent."
	\item \textbf{Intended Victim.} Sarcastic comments can be used to mock a victim, and insincere questions can be discriminative or disrespectful against certain groups of people. The harmful components in both situations may be implicit or explicit. 
	\item \textbf{Violation of Truthfulness.} Sarcastic comments that violate truthfulness resemble insincere questions that are based on false premises. To understand such languages, the listener needs to know what is the true background information and how the truthfulness is violated\citep{joshi2017}. This can be a hard problem in natural language processing as it largely depends on knowledge about certain people or certain topics. However, it is also possible that the untruthful texts have syntactic or semantic characteristics that can become features of machine learning classification models.  
\end{enumerate}
% Features in textual space vs embedding space

\subsection{Statistics-Focused Features} \label{statistic-features}

 Feature selection in sentiment classification follows an established pattern where four types of features are frequently examined, namely term frequencies, parts of speech counts, the presence of opinion words, and negations\citep{medhat2014}. A particularly relevant study was conducted by \citet{barbosa2010}, where the authors presented a 2-step sentiment analysis method that only utilized an SVM model for the machine learning process, but its feature extraction methods allowed for robustness against noisy data. The researchers developed two feature sets designed to provide abstract representations of short texts: meta-features (including PoS tags and prior subjectivity and polarity) and syntactic features (namely frequencies of certain types of characters). The detection process was divided into two parts: subjectivity detection and polarity detection of the subjective texts. The experiment followed a traditional approach where the classifiers with highest information gain were created using Weka, and the learning process was completed using SVM algorithm\citep{barbosa2010}. Although this method no longer offers state-of-art performance, we still consider the feature selection methods valuable to our project. We intend to adapt the design in this study to obtain measurements of corpus statistics to feed as features into our neural network models. 

\subsection{Word Embeddings} \label{word-embeddings}
Before we feed the data into our models, we need to project it into vectors of real numbers. Therefore, we review some recent techniques performing word embeddings and present here.

\textbf{Neural Network Language Model.} A critical obstacle of language modeling is caused by the curse of dimensionality. To address this problem, in 2003, \citet{bengio2003neural} proposed a feed-forward Neural Network Language Model (NNLM) to learn a densely distributed representation\citep{hinton1986learning} for words, where both the word feature vectors and the parameters of that probability function were learned at the same time. NNLM is designed to learn $f \left( w _ { t } , \cdots , w _ { t - n + 1 } \right) = \hat { P } \left( w _ { t } | w _ { 1 } ^ { t - 1 } \right)$, $w _ { t } \in V$, where $w_1 \cdots w_T$ is the word sequence and $V$ is a large but finite vocabulary. This objective is divided into two steps: firstly, the previous N words would be mapped into real vectors by a share projection matrix, which represents each word of the vocabulary with a distributed feature vectors; then, the sequence of word feature vectors would be mapped into a probability distribution over $V$. As a result, this model yielded better perplexity compared to N-gram models and inspired more researchers to look into distributed representations of words.

\textbf{Word2Vec.} Recognizing the need to include distributed representations of words introduced by \citet{hinton1984distributed}, \citet{mikolov2013efficient} analyzed NNLM and proposed two models to learn the words' continuous vector representations. Originally, NNLM mainly consisted of three layers: words were first mapped into embeddings at projection layer; then embedded vectors flowed into an ordinary hyperbolic tangent hidden layer; finally, the output layer represented the probability distribution over $V$. Therefore, most of the computation happened in the output layer. To learn the continuous word representations with higher accuracy and lower computational cost from a huge corpus, the first model Mikolov et al. proposed was the Continuous Bag-of-Words Model (CBOW) which removed the non-linear hidden layer. In CBOW, embedded words would be first projected and then averaged into the same position, after which an output layer would be presented with hierarchical softmax. They also provided another model that was similar to CBOW called Continuous Skip-gram Model to predict the context given a word. The result turned out to be promising as the model did not only extract the syntactic regularities but also revealed subtle semantic information of words.

\textbf{Global Vectors for Word Representations.} Although methods for word embeddings claimed great success in capturing syntactic and semantic properties, \citet{pennington2014glove} found that there were still drawbacks in the two major models: global matrix factorization and local context window methods. For instance, global matrix factorization performs poorly in word analogy task, while local context window methods like skip-gram fail to exploit the statistical information provided by copus. Therefore, they proposed a log-bilinear regression model that combines the advantages of the two models called Global Vectors for Word Representations (GloVe), as the global corpus information are properly captured in this model. In designing their model, they first constructed a word-word co-occurance matrix $X$, where $X_{ij}$ refers to the number of times word $j$ appears in the context of word $i$. Then they defined $X_i=\sum_kX_{ik}$ indicating the number of times word $i$ serves as the context for any other word $k$. Finally, they let $P_{ij}=P(j|i)=X_{ij}/X_i$ be the probability of word $j$ appeared in the context of word $i$. With the ratios of co-occurance probabilities, they found that certain aspects of meaning could be well captured, therefore they encoded it into GloVe and achieved higher accuracy with lower computational cost.


A study on word embedding-based features \citep{joshi2016} provides insights on how to combine word embeddings with feature selection. This work examined how features calculated from word embedding vectors could augment existing feature sets including n-grams, dictionary-based features, syntactical features and pattern-based features. More specifically, the researchers attempted to use word vector distances to detect context incongruity independent of sentiment changes. The results indicated that word embedding-based features enhanced performance \citep{joshi2016}. This method might be useful in our project for two reasons: 1) insincere questions may also contain context incongruity because of conflict between asking a question while making a statement; 2) questions that have discriminative or sexual language may be detected based on the semantic similarities to a set of sensitive keywords.  In this system, known sources of errors include multiple-sense-induced embedding issues, incapability to identify contextual information, and non-sarcastic metaphors\citep{joshi2016}. We expect the latter two points have lower impact on insincerity detection than sarcasm detection because contextual information and metaphors are less relevant to insincere questions than sarcastic messages. 

\subsection{Neural Networks Models} \label{nn-models}

% for citation, use \citep{} instead of \cite

Here we review several studies on sarcasm detection and sentiment analysis in the hope of transferring core concepts and methods to insincere question classification.

Compared to long texts that contain more contextual information, short texts sentiment analysis can be more challenging, and thus methods such as bag-of-words or n-grams are shown to be less effective \citep{barbosa2010, santos2014}. \citep{santos2014} proposed a deep CNN model that constructed word embeddings from character-level to sentence-level for sentiment analysis. The character-level embeddings captured morphological information and the word-level embeddings catch syntactical and semantical information \citep{santos2014}. Finally, the sentence-level representations were constructed upon concatenated character-level and word-level features. The word-level embeddings came from Word2Vec embeddings, and the character-level and sentence-level were obtained using convolutional layers that successively extracted local features in windows and maxed over these windows to get the global representation \citep{santos2014}. According to the results, the extracted features at sentence-level typically concentrated on several sentiment-bearing keywords. On the other hand, the contributions of character-level representations were not closely examined, so to what degree morphological and shape information benefited sentiment analysis was not clear. Our project will primarily focus on semantical and syntactical analysis, but we would also like to explore the advantages of character-level embeddings given enough time and resources. 

\citep{poria2017} presented a CNN architecture that contained in parallel a baseline 2-layer CNN model that directly process the text and three pre-trained models covering three types of clues (in particular, sentiments, emotions and personalities) that could be beneficial for sarcasm detection. These pre-trained models were trained on their benchmark datasets and then used for feature extraction on the sarcastic tweets datasets. Unlike the CNN model that specialized in obtaining textual information in depth \citep{santos2014}, the four-component CNN model in \citep{poria2017} horizontally extracted information from different aspects. Although the baseline features (directly extracted from the word vectors) showed better predictions than all pre-trained models, the combination of the baseline and pre-trained models yielded the best performance \citep{poria2017}. The result indicated that compared to completely distributing word representations, modularized components with specific tasks could improve the overall performance of a CNN model. As we discussed in the previous sections, we have established several characteristics that indicate a Quora question to be insincere; therefore, the idea of combining various components each responsible for detecting different classification clues can be valuable for our project. To merge the outputs of the four parts, the researchers tested two methods: combining the outputs using SVM and appending extracted features from the pre-trained models into the baseline CNN hidden layers \citep{poria2017}. The former method showed better results possibly because appending the extracted features into the baseline models compromised their meaningfulness. Also, SVM classifiers are known to be ideal for processing text data due to the sparse nature of text \citep{medhat2014}. One additional lesson from this study is that their CNN model with a relatively lower feature dimension (at 1,100) outperformed models using larger vector size (>500,000) both in terms of accuracy and computation speed \citep{poria2017}. 

Another sarcasm detection study \citep{ghosh2016} presented a CNN-LSTM-DNN neural networks model for semantic modeling, and this model architecture was particularly relevant to our project design. Consider that the information-bearing parts of text could exist anywhere, the choice of convolutional neural networks and recurrent neural networks was reasonable as both CNN and RNN were capable of capturing sequential temporal information. Among RNN implementations, LSTMs were beneficial because of the easiness to train and the avoidance of vanishing or exploding gradients. The combination of CNN and RNN networks were also advantageous for the several reasons. First, convolutional layer helped to extract abstract and compact features to be used as inputs to LSTM network \citep{chan2015}, which in turn compensates for the disadvantage of fixed filter width of convolutional layers, allowing for better analysis of long-distance relationships over texts with various length. Additionally, CNN could reduce feature variations so that CNN-LSTM worked more efficiently \citep{ghosh2016}. On top of the LSTM layers, the researchers added a fully-connected DNN layer to map the features into a more separable space. The combination of CNN, LSTM, and DNN showed more superior results than recursive SVM, two-layer CNN, and two-layer RNN models \citep{ghosh2016}. Consider the similarity between the tasks of this study and our project, we intend to construct our neural networks architecture following this CNN-LSTM pattern. 

\newpage


%------------------------------------------------

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}

\bibliography{csc2511.bib}

%----------------------------------------------------------------------------------------

\end{document}
